# 项目周报

日期：2025-5-18

项目实践题目：文本向量化的高级技术

## 实践内容

### 算法理论深入学习
- （跳出KDTree/BallTree框架，学习更适合高维的近似算法。）

#### ANNOY（Approximate Nearest Neighbors Oh Yeah）
- 原理：基于层次化分区树（Hierarchical Partition Trees），通过随机投影将高维空间划分为子空间，构建森林结构。查询时在多棵树中贪心搜索最近邻，通过调整搜索深度（n_trees和search_k）平衡精度与速度。
- 特点：适合高维向量（如词向量），构建速度快，内存占用低，支持动态数据插入。
- 阅读了解目前优化程度：论文《ANNOY+：Adaptive Nearest Neighbor Search with Learned Navigable Trees》（SIGIR 2022）。
- 改进点：
1. 动态树调整：根据查询反馈合并/分裂树节点。
2. 混合距离度量：在叶子节点结合欧氏距离和余弦距离。
- 效果：在 GloVe 300 维数据上，召回率从 70% → 85%（相同查询耗时）

#### HNSW（Hierarchical Navigable Small World graphs）
- 原理：基于小世界图（Small World Graphs）和层次结构，底层为全连接图，高层为稀疏图。查询时从高层快速跳转至近似区域，逐层细化搜索。
- 特点：高维性能优异（优于 BallTree 和 KDTree），精度高，但构建时间长（需调参M和efConstruction）。
- 阅读了解目前优化程度：HNSW 原始论文《Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs》引入分层构建策略与自适应参数调整，使 HNSW 在高维数据（>500 维）上构建时间缩短 40%，且召回率稳定在 93% 以上，突破传统 HNSW 在高维场景的性能瓶颈。
- HNSW参数调优：
- 阅读论文《Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs》的参数调优章节。
1. 分层结构参数：HNSW增量式构建一个多层结构，元素所在的最大层数是根据指数衰减概率分布随机选择的。这种分层结构及层数选择方式影响搜索性能，通过从上层开始搜索并利用尺度分离，能提升性能并实现对数复杂度扩展。
2. 邻居选择启发式参数：采用了一种启发式方法来选择邻近图的邻居，在高召回率和数据高度聚类的情况下，能显著提高性能。

### 收获体会
- 减少对微小优化的执念，关注算法前沿进展
1.	理论边界认知：KDTree/BallTree 是精确算法，适合小规模或低维数据；ANNOY/HNSW 是近似算法，需用召回率、查询时间、内存占用三维度评估。
2.	参数敏感性：近似算法的性能高度依赖参数（如 HNSW 的M），需通过交叉验证。
3.	不停留于表面优化：本周聚焦于算法原理差异（如 HNSW 的层次图 vs ANNOY 的分区树），而非纠结于代码级优化。

### 下一步计划
- 技术深化
1. ANNOY 动态插入实验：在已构建的树结构中新增 1 万条数据，测试插入耗时与查询性能衰减情况（预期插入速度快，但旧数据查询精度可能下降）。
2. HNSW 量化压缩：结合 PQ（Product Quantization）将 300 维向量压缩至 64 字节，对比压缩前后的内存占用与召回率损失（目标：内存压缩 50%，召回率保持 85% 以上）。
- 扩展学习：学习 IVF（Inverted File Index） 原理，阅读 Faiss 官方文档中 “IVF+PQ” 组合的应用案例。






